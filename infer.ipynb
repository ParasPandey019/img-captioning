{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import io\n",
    "\n",
    "# Import custom modules\n",
    "from encoder import EncoderCNN\n",
    "from decoder import DecoderRNN\n",
    "from vocab import Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90616086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model parameters\n",
    "model_path = './models/final_model.pth'\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    embed_size = checkpoint['embed_size']\n",
    "    hidden_size = checkpoint['hidden_size']\n",
    "    vocab_size = checkpoint['vocab_size']\n",
    "    \n",
    "    print(f\"Model parameters loaded:\")\n",
    "    print(f\"Embed size: {embed_size}\")\n",
    "    print(f\"Hidden size: {hidden_size}\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "else:\n",
    "    # Fallback to individual model files\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    print(\"Using default parameters and loading individual model files...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4b047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocab_file = \"vocab.pkl\"\n",
    "with open(vocab_file, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "if 'vocab_size' not in locals():\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary loaded with {vocab_size} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de835f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load models\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Load model weights\n",
    "if os.path.exists(model_path):\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "else:\n",
    "    # Load from individual files\n",
    "    encoder.load_state_dict(torch.load('./models/encoder-3.pkl', map_location=device))\n",
    "    decoder.load_state_dict(torch.load('./models/decoder-3.pkl', map_location=device))\n",
    "\n",
    "# Set to evaluation mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(\"Models loaded and set to evaluation mode!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "def clean_sentence(output, idx2word):\n",
    "    \"\"\"Convert word indices to clean sentence.\"\"\"\n",
    "    sentence = \"\"\n",
    "    for i in output:\n",
    "        if i == 0:  # <pad> token\n",
    "            continue\n",
    "        if i == 1:  # <end> token\n",
    "            break\n",
    "        word = idx2word[i]\n",
    "        if i == 18:  # Handle punctuation\n",
    "            sentence = sentence + word\n",
    "        else:\n",
    "            sentence = sentence + \" \" + word\n",
    "    return sentence.strip()\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    \"\"\"Generate caption for an image.\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    with torch.no_grad():\n",
    "        features = encoder(image_tensor).unsqueeze(1)\n",
    "        sampled_ids = decoder.sample(features)\n",
    "        caption = clean_sentence(sampled_ids, vocab.idx2word)\n",
    "    \n",
    "    return image, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9790ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file upload widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='image/*',  # Accept only image files\n",
    "    multiple=False,    # Single file upload\n",
    "    description='Upload Image'\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def on_upload_change(change):\n",
    "    \"\"\"Handle file upload.\"\"\"\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if upload_widget.value:\n",
    "            # Get uploaded file\n",
    "            uploaded_file = list(upload_widget.value.values())[0]\n",
    "            \n",
    "            # Save temporarily\n",
    "            temp_path = 'temp_image.jpg'\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                f.write(uploaded_file['content'])\n",
    "            \n",
    "            try:\n",
    "                # Generate caption\n",
    "                image, caption = generate_caption(temp_path)\n",
    "                \n",
    "                # Display results\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.imshow(image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Generated Caption: {caption}', fontsize=14, pad=20)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"\\nðŸ“¸ Caption: {caption}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "            \n",
    "            finally:\n",
    "                # Clean up temporary file\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "\n",
    "# Set up the upload widget\n",
    "upload_widget.observe(on_upload_change, names='value')\n",
    "\n",
    "# Display widgets\n",
    "display(upload_widget, output_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample image if available\n",
    "sample_images = ['test.jpg', 'sample.jpg', 'example.jpg']\n",
    "\n",
    "for img_path in sample_images:\n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"Testing with {img_path}...\")\n",
    "        try:\n",
    "            image, caption = generate_caption(img_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Generated Caption: {caption}', fontsize=12, pad=15)\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Caption: {caption}\\n\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {img_path}: {e}\")\n",
    "else:\n",
    "    print(\"No sample images found. Please upload an image using the widget above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple images from a directory\n",
    "def process_image_directory(image_dir, max_images=5):\n",
    "    \"\"\"Process multiple images from a directory.\"\"\"\n",
    "    if not os.path.exists(image_dir):\n",
    "        print(f\"Directory {image_dir} not found.\")\n",
    "        return\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_files = [f for f in os.listdir(image_dir) \n",
    "                   if any(f.lower().endswith(ext) for ext in image_extensions)]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No image files found in {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {min(len(image_files), max_images)} images...\")\n",
    "    \n",
    "    for i, img_file in enumerate(image_files[:max_images]):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        try:\n",
    "            image, caption = generate_caption(img_path)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'{img_file}: {caption}', fontsize=10)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "# Example usage (uncomment and modify path as needed)\n",
    "# process_image_directory('./test_images/', max_images=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6881ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced caption generation with confidence scores\n",
    "def generate_caption_with_confidence(image_path, max_len=20):\n",
    "    \"\"\"Generate caption with word confidence scores.\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = encoder(image_tensor).unsqueeze(1)\n",
    "        \n",
    "        # Manual sampling with confidence\n",
    "        sampled_ids = []\n",
    "        confidences = []\n",
    "        inputs = features\n",
    "        states = None\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            hiddens, states = decoder.lstm(inputs, states)\n",
    "            outputs = decoder.linear(hiddens.squeeze(1))\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            predicted_prob, predicted_idx = probs.max(1)\n",
    "            \n",
    "            sampled_ids.append(predicted_idx.item())\n",
    "            confidences.append(predicted_prob.item())\n",
    "            \n",
    "            if predicted_idx == 1:  # <end> token\n",
    "                break\n",
    "                \n",
    "            inputs = decoder.embed(predicted_idx).unsqueeze(1)\n",
    "        \n",
    "        caption = clean_sentence(sampled_ids, vocab.idx2word)\n",
    "        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
    "    \n",
    "    return image, caption, avg_confidence\n",
    "\n",
    "# Example usage function\n",
    "def test_with_confidence(image_path):\n",
    "    \"\"\"Test caption generation with confidence score.\"\"\"\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            image, caption, confidence = generate_caption_with_confidence(image_path)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Caption: {caption}\\nConfidence: {confidence:.2f}', fontsize=12, pad=15)\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Caption: {caption}\")\n",
    "            print(f\"Average Confidence: {confidence:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(f\"Image {image_path} not found\")\n",
    "\n",
    "# Uncomment to test with a sample image\n",
    "# test_with_confidence('test.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a87636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information and statistics\n",
    "print(\"=== Model Information ===\")\n",
    "print(f\"Encoder: CNN (ResNet-50 based)\")\n",
    "print(f\"Decoder: LSTM\")\n",
    "print(f\"Embedding Size: {embed_size}\")\n",
    "print(f\"Hidden Size: {hidden_size}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Count model parameters\n",
    "encoder_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "decoder_params = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "total_params = encoder_params + decoder_params\n",
    "\n",
    "print(f\"\\n=== Model Parameters ===\")\n",
    "print(f\"Trainable Encoder Parameters: {encoder_params:,}\")\n",
    "print(f\"Decoder Parameters: {decoder_params:,}\")\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
