{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8265885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bffd2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCaptionDataset(Dataset):\n",
    "    def __init__(self,img_folder,annotation_file,transform=None,vocab=None):\n",
    "        self.img_folder =img_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "\n",
    "        # Id to file mapping\n",
    "        self.img_id_to_filename = {img['id']: img['file_name'] for img in self.coco_data['images']}\n",
    "\n",
    "        # All captions with image ids\n",
    "        self.captions = self.coco_data['annotations']\n",
    "\n",
    "        # vocab for tokenizing captions\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "\n",
    "    def __get_item__(self,idx):\n",
    "        caption = self.captions[idx]['caption']\n",
    "        img_id = self.captions[idx]['image_id']\n",
    "        img_filename = self.img_id_to_filename[img_id]\n",
    "        img_path = os.path.join(self.img_folder, img_filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.vocab:\n",
    "            caption_tokens = [self.vocab['<start>']] + [self.vocab.get(word, self.vocab['<unk>']) for word in caption.lower().split()] + [self.vocab['<end>']]\n",
    "            caption_tokens = torch.tensor(caption_tokens)\n",
    "            return image, caption_tokens\n",
    "        else:\n",
    "            return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67445081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgcptn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
