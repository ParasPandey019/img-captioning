{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import custom modules\n",
    "from encoder import EncoderCNN\n",
    "from decoder import DecoderRNN\n",
    "from dataset import get_loader\n",
    "from vocab import Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37fcc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP utilities\n",
    "def clean_sentence(output, idx2word):\n",
    "    \"\"\"Convert word indices to clean sentence.\"\"\"\n",
    "    sentence = \"\"\n",
    "    for i in output:\n",
    "        if i == 0:  # <pad> token\n",
    "            continue\n",
    "        if i == 1:  # <end> token\n",
    "            break\n",
    "        word = idx2word[i]\n",
    "        if i == 18:  # Handle punctuation\n",
    "            sentence = sentence + word\n",
    "        else:\n",
    "            sentence = sentence + \" \" + word\n",
    "    return sentence.strip()\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def bleu_score(true_sentences, predicted_sentences):\n",
    "    \"\"\"Calculate BLEU score.\"\"\"\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for img_id in set(true_sentences.keys()).intersection(set(predicted_sentences.keys())):\n",
    "        img_refs = [cap.split() for cap in true_sentences[img_id]]\n",
    "        references.append(img_refs)\n",
    "        hypotheses.append(predicted_sentences[img_id][0].strip().split())\n",
    "    return corpus_bleu(references, hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 128\n",
    "vocab_threshold = 5\n",
    "vocab_from_file = True\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 3\n",
    "save_every = 1\n",
    "print_every = 20\n",
    "log_file = \"training_log.txt\"\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset location\n",
    "cocoapi_dir = \"./coco2017/\"\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Validation/test transformations\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=cocoapi_dir\n",
    ")\n",
    "\n",
    "val_loader = get_loader(\n",
    "    transform=transform_test,\n",
    "    mode='valid',\n",
    "    cocoapi_loc=cocoapi_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf58ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size\n",
    "vocab_size = len(train_loader.dataset.vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Initialize encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Optimizer (only train decoder and encoder embedding layer)\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Training steps per epoch\n",
    "total_step = math.ceil(len(train_loader.dataset) / train_loader.batch_sampler.batch_size)\n",
    "print(f\"Total training steps per epoch: {total_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8694e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "with open(log_file, 'w') as f:\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for i_step in range(1, total_step + 1):\n",
    "            \n",
    "            # Get batch with same caption length\n",
    "            indices = train_loader.dataset.get_train_indices()\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            train_loader.batch_sampler.sampler = new_sampler\n",
    "            \n",
    "            # Get batch\n",
    "            images, captions = next(iter(train_loader))\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log statistics\n",
    "            stats = (f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "                    f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\")\n",
    "            \n",
    "            f.write(stats + \"\\n\")\n",
    "            f.flush()\n",
    "            \n",
    "            if i_step % print_every == 0:\n",
    "                print(stats)\n",
    "        \n",
    "        # Save model weights\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save(decoder.state_dict(), f'./models/decoder-{epoch}.pkl')\n",
    "            torch.save(encoder.state_dict(), f'./models/encoder-{epoch}.pkl')\n",
    "            print(f\"Model saved at epoch {epoch}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "encoder_file = f\"encoder-{num_epochs}.pkl\"\n",
    "decoder_file = f\"decoder-{num_epochs}.pkl\"\n",
    "\n",
    "encoder.load_state_dict(torch.load(f'./models/{encoder_file}'))\n",
    "decoder.load_state_dict(torch.load(f'./models/{decoder_file}'))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(\"Model loaded and set to evaluation mode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for validation set\n",
    "pred_result = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_id, img in tqdm(val_loader, desc=\"Generating captions\"):\n",
    "        img = img.to(device)\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "        sentence = clean_sentence(output, val_loader.dataset.vocab.idx2word)\n",
    "        pred_result[img_id.item()].append(sentence)\n",
    "\n",
    "print(f\"Generated captions for {len(pred_result)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth captions\n",
    "with open(os.path.join(cocoapi_dir, \"annotations/captions_val2017.json\"), \"r\") as f:\n",
    "    caption_data = json.load(f)\n",
    "\n",
    "valid_annot = caption_data[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for annotation in valid_annot:\n",
    "    valid_result[annotation[\"image_id\"]].append(annotation[\"caption\"].lower())\n",
    "\n",
    "print(f\"Loaded ground truth captions for {len(valid_result)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "bleu = bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)\n",
    "print(f\"BLEU Score: {bleu:.4f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExample predictions:\")\n",
    "for i, (img_id, preds) in enumerate(list(pred_result.items())[:3]):\n",
    "    print(f\"\\nImage {img_id}:\")\n",
    "    print(f\"Predicted: {preds[0]}\")\n",
    "    if img_id in valid_result:\n",
    "        print(f\"Ground truth: {valid_result[img_id][:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cbf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final trained model\n",
    "torch.save({\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'embed_size': embed_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'vocab_size': vocab_size,\n",
    "    'vocab_threshold': vocab_threshold\n",
    "}, './models/final_model.pth')\n",
    "\n",
    "print(\"Final model saved as 'final_model.pth'\")\n",
    "print(f\"Model achieved BLEU score of {bleu:.4f} on validation set\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgcptn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
